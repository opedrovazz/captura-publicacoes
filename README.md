# ğŸ“œ Captura de PublicaÃ§Ãµes Legais

Projeto em **Python + Flask** para realizar **web scraping** de publicaÃ§Ãµes legais disponÃ­veis nos portais:

- [DiÃ¡rio do ComÃ©rcio](https://diariodocomercio.com.br/publicidade-legal-impresso/)  
- [DiÃ¡rio Comercial](https://diariocomercial.com.br/publicidade-legal/)  
- [Agora RN](https://agorarn.com.br/publicacoescertificadas/)

O sistema coleta publicaÃ§Ãµes legais, aplica filtros opcionais, exporta resultados em **JSON** ou **CSV**, e permite execuÃ§Ã£o **tanto via API Flask quanto via linha de comando (CLI)**.

---

## ğŸ§  Funcionalidades Principais

- Coleta automÃ¡tica de publicaÃ§Ãµes por site.  
- **Filtro obrigatÃ³rio de data mÃ¡xima** (`--date dd/mm/yyyy`).  
- **Filtro opcional de tÃ­tulo** (`--filter-text "palavra"`), insensÃ­vel a acentos e caixa.  
- ExportaÃ§Ã£o de resultados em **JSON** (padrÃ£o) ou **CSV**.  
- **API local Flask** para consulta via HTTP.  
- ExecuÃ§Ã£o **via linha de comando** ou **modo servidor/scheduler**.  
- LÃ³gica comum centralizada em `BaseScraper`, evitando duplicaÃ§Ã£o de cÃ³digo.

---

## ğŸ§± Estrutura do Projeto

captura-publicacoes/
â”‚
â”œâ”€â”€ main.py                         # Ponto de entrada principal (CLI, API e scheduler)
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ base_scraper.py             # Classe base genÃ©rica
â”‚   â””â”€â”€ sites/
â”‚       â”œâ”€â”€ diariodocomercio/
â”‚       â”‚   â”œâ”€â”€ diariodocomercio_integration.py
â”‚       â”‚   â””â”€â”€ diariodocomercio_service.py
â”‚       â”œâ”€â”€ diariocomercial/
â”‚       â”‚   â”œâ”€â”€ diariocomercial_integration.py
â”‚       â”‚   â””â”€â”€ diariocomercial_service.py
â”‚       â””â”€â”€ agorarn/
â”‚           â”œâ”€â”€ agorarn_integration.py
â”‚           â””â”€â”€ agorarn_service.py
â”‚
â”œâ”€â”€ api/                            # Rotas e inicializaÃ§Ã£o da API Flask
â”‚   â””â”€â”€ routes.py
â”œâ”€â”€ scheduler/                      # Tarefas agendadas de scraping automÃ¡tico
â”‚   â””â”€â”€ init.py
â””â”€â”€ tests/                          # Testes

---

## âš™ï¸ InstalaÃ§Ã£o

```bash
git clone https://github.com/opedrovazz/captura-publicacoes.git
cd captura-publicacoes
pip install -r requirements.txt
```

---

## ğŸš€ Modo de ExecuÃ§Ã£o via Linha de Comando (CLI)

Permite rodar os scrapers manualmente com parÃ¢metros de filtro e formato.

### Sintaxe geral
```bash
python main.py <site> --date dd/mm/yyyy [--format json|csv] [--filter-text "palavra"]
```

### ParÃ¢metros

| ParÃ¢metro | ObrigatÃ³rio | DescriÃ§Ã£o |
|------------|-------------|------------|
| `<site>` | âœ… | Nome do site (`agorarn`, `diariodocomercio`, `diariocomercial`) |
| `--date` | âœ… | Data limite no formato `dd/mm/yyyy`. PublicaÃ§Ãµes posteriores sÃ£o ignoradas. |
| `--format` | âŒ | Formato de saÃ­da: `json` (padrÃ£o) ou `csv`. |
| `--filter-text` | âŒ | Palavra a ser buscada nos tÃ­tulos (case-insensitive e sem acentos). |

### Exemplos

**1ï¸âƒ£ Executar o scraper do DiÃ¡rio do ComÃ©rcio (JSON padrÃ£o)**  
```bash
python main.py diariodocomercio --date 31/10/2025
```

**2ï¸âƒ£ Exportar CSV filtrando tÃ­tulos que contenham â€œassembleiaâ€**  
```bash
python main.py diariocomercial --date 30/10/2025 --format csv --filter-text "assembleia"
```

**3ï¸âƒ£ Rodar para Agora RN filtrando â€œdemonstrativoâ€**  
```bash
python main.py agorarn --date 29/10/2025 --filter-text "demonstrativo"
```

### SaÃ­da gerada

O scraper salva automaticamente o resultado no diretÃ³rio atual:

```
resultados_<site>_<data>.json
resultados_<site>_<data>.csv
```

Exemplo:
```
resultados_diariodocomercio_31-10-2025.csv
```

Cada registro contÃ©m:
```json
{
  "date": "30/10/2025",
  "title": "BalanÃ§o Patrimonial 2024",
  "pdf_url": "https://...",
  "site": "diariodocomercio.com.br",
  "original_url": "https://..."
}
```

---

## ğŸ§© Modo Servidor (API Flask)

Permite rodar o servidor Flask local para consultas via HTTP:

```bash
python main.py api
```

Servidor padrÃ£o:
```
http://127.0.0.1:5000
```

### Rotas disponÃ­veis

**GET /**  
Retorna informaÃ§Ãµes sobre a API e os sites suportados.

**GET /<site>?date=dd/mm/yyyy&format=json|csv&filter=palavra**  
Executa o scraper e retorna o resultado direto na resposta.

**Exemplo:**
```
GET /agorarn?date=29/10/2025&format=csv&filter=assembleia
```

**Resposta (CSV):**
```
date,title,pdf_url,site,original_url
29/10/2025,Assembleia Geral ExtraordinÃ¡ria,https://...,agorarn.com.br,https://...
```

---

## ğŸ•’ Modo Agendado (Scheduler)

ExecuÃ§Ã£o automÃ¡tica diÃ¡ria dos scrapers:
```bash
python main.py scheduler
```

Ou para rodar API + Scheduler simultaneamente:
```bash
python main.py both
```

---

## ğŸ§° Tecnologias e Bibliotecas Utilizadas

- **Python 3.10+**  
- **Flask** â€” API HTTP  
- **BeautifulSoup4 + lxml** â€” parsing HTML  
- **urllib** â€” requisiÃ§Ãµes HTTP  
- **threading** â€” execuÃ§Ã£o simultÃ¢nea da API e agendador  
- **argparse** â€” interface CLI

---

## ğŸ“ Formatos de SaÃ­da

| Formato | ExtensÃ£o | DescriÃ§Ã£o |
|----------|-----------|-----------|
| JSON | `.json` | Lista de objetos com `date`, `title`, `pdf_url`, `site`, `original_url`. |
| CSV | `.csv` | Arquivo tabular com cabeÃ§alho `date,title,pdf_url,site,original_url`. |

---

## âœ… Escopo de Coleta

O scraper coleta todas as publicaÃ§Ãµes disponÃ­veis de 2025 em cada site.  
Ele interrompe automaticamente quando atinge uma publicaÃ§Ã£o posterior Ã  data limite (`--date`).

---

## ğŸ§¾ Exemplo (CLI)

```bash
python main.py agorarn --date 31/10/2025 --format csv --filter-text "patrimonial"
```

SaÃ­da esperada:
```
Iniciando coleta para o site 'agorarn' com data limite 31/10/2025...
Processando pÃ¡gina: https://agorarn.com.br/publicacoescertificadas/page/1/
5 publicaÃ§Ãµes encontradas.
Coletado: 29/10/2025 - BalanÃ§o Patrimonial 2024
âœ… Coleta concluÃ­da. Resultado salvo em: resultados_agorarn_31-10-2025.csv
```

